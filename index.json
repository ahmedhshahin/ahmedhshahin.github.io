[{"authors":null,"categories":null,"content":"My name is Ahmed, and I am a PhD student in the Department of Computer Science at UCL. I am supervised by David Barber at the UCL AI Centre and Daniel Alexander at the UCL Centre for Medical Image Computing. My PhD research focuses on developing machine learning methods for the prognosis and diagnosis of Interstitial Lung Diseases from medical images and other clinical information, as part of the Open Source Imaging Consortium (OSIC) efforts to enable rapid advances in the fight against respiratory diseases.\nStarting from August 2022, I am doing a research internship at Siemens Healthineers, Princeton, USA. As part of the Cardiovascular Imaging team, I am designing machine learning methods to assist clinicians in the diagnosis and treatment of cardiovascular diseases.\nBefore UCL, I worked as a senior machine learning engineer at Intixel (Cairo-based startup for AI in radiology) and spent a six-month research internship at the Inception Institute of AI in Abu Dhabi. I have first-author publications in MICCAI, MIDL, RSNA, and ISBI conferences. I also served as a reviewer for MICCAI, the International Journal of Computer Vision (IJCV), IEEE Access, and NILES'19. Further, I am a program committe member of Medical Imaging Meets NeurIPS and FAIR-MICCAI workshops.\n Download my CV (last updated 2/3/2022).\n","date":1646006400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1646006400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"My name is Ahmed, and I am a PhD student in the Department of Computer Science at UCL. I am supervised by David Barber at the UCL AI Centre and Daniel Alexander at the UCL Centre for Medical Image Computing.","tags":null,"title":"Ahmed H. Shahin","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Ahmed H. Shahin","Joseph Jacob","Daniel C. Alexander","David Barber"],"categories":[],"content":"","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"4e9a3488b920da14e1b81a0d34259715","permalink":"/publication/midl22/","publishdate":"2022-02-28T16:59:59.637688Z","relpermalink":"/publication/midl22/","section":"publication","summary":"Idiopathic Pulmonary Fibrosis (IPF) is an inexorably progressive fibrotic lung disease with a variable and unpredictable rate of progression. CT scans of the lungs inform clinical assessment of IPF patients and contain pertinent information related to disease progression. In this work, we propose a multi-modal method that uses neural networks and memory banks to predict the survival of IPF patients using clinical and imaging data. The majority of clinical IPF patient records have missing data (e.g. missing lung function tests). To this end, we propose a probabilistic model that captures the dependencies between the observed clinical variables and imputes missing ones. This principled approach to missing data imputation can be naturally combined with a deep survival analysis model. We show that the proposed framework yields significantly better survival analysis results than baselines in terms of concordance index and integrated Brier score. Our work also provides insights into novel image-based biomarkers that are linked to mortality.","tags":["deep learning","survival analysis","IPF","interstitial lung diseases","neural networks"],"title":"Survival Analysis for Idiopathic Pulmonary Fibrosis using CT Images and Incomplete Clinical Data","type":"publication"},{"authors":["Ghadir Ali","Ahmed H. Shahin","Mohamed Elhadidi","Mustafa Elattar"],"categories":[],"content":"","date":1608422400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608422400,"objectID":"b27e480cd21eb5c1afd1d53d1b552c35","permalink":"/publication/3ict/","publishdate":"2022-02-28T16:59:59.637688Z","relpermalink":"/publication/3ict/","section":"publication","summary":"In 2017, pneumonia was the primary diagnosis for 1.3 million visits to the Emergency Department (ED) in the United States. The mortality rate was estimated to be 5%-10% of hospitalized patients, whereas it rises to 30% for severe cases admitted to the Intensive Care Unit (ICU). Among all cases admitted to ED, 30% were misdiagnosed, and they did not suffer from pneumonia, which raises a flag for the need for more accurate diagnosis methods. Several methods for pneumonia detection were recently developed using AI in general and more specifically, using deep neural networks. Even though it worth acknowledging the significant limitations and concerns on the generalizability of such models and the barriers facing the employment of this technology for clinical practice. In this paper, an Attention model is used with a Convolutional Neural Network (CNN) for lung pneumonia diagnosis. The backbone of the model is a ResNet50 architecture with an added dual attention layer. The model was trained on the chest x-ray dataset for the aim of chest pneumonia classification. The model achieved an average validation accuracy of 97.82% and AUROC of 0.98842 on our split with cross validation. Regarding the original split, accuracy was 77.63% and AUROC of 0.7967 on the official test set. In summary, incorporation of established computer vision techniques such as Attention modules seems to be a promising approach for advancing medical image analysis.","tags":["medical diagnostic imaging","computer vision"],"title":"Convolutional Neural Network with Attention Modules for Pneumonia Detection","type":"publication"},{"authors":["Ahmed H. Shahin","Âê≥ÊÅ©ÈÅî"],"categories":["Demo","ÊïôÁ®ã"],"content":"Overview  The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more    The template is mobile first with a responsive design to ensure that your site looks stunning on every device.  Get Started  üëâ Create a new site üìö Personalize your site üí¨ Chat with the Wowchemy community or Hugo community üê¶ Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy üí° Request a feature or report a bug for Wowchemy ‚¨ÜÔ∏è Updating Wowchemy? View the Update Guide and Release Notes  Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n‚ù§Ô∏è Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ‚ù§Ô∏è As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features ü¶Ñ‚ú®\nEcosystem  Hugo Academic CLI: Automatically import publications from BibTeX  Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures  Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, ‰∏≠Êñá, and Portugu√™s Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome üëã We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","ÂºÄÊ∫ê"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Ahmed H. Shahin","Prateek Munjal","Ling Shao","Shadab Khan"],"categories":[],"content":"","date":1585958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585958400,"objectID":"855ac75ad8b7ab3656b53efcc6596ac2","permalink":"/publication/arxiv2020/","publishdate":"2022-02-28T16:59:59.637688Z","relpermalink":"/publication/arxiv2020/","section":"publication","summary":"Semantic segmentation from user inputs has been actively studied to facilitate interactive segmentation for data annotation and other applications. Recent studies have shown that extreme points can be effectively used to encode user inputs. A heat map generated from the extreme points can be appended to the RGB image and input to the model for training. In this study, we present FAIRS -- a new approach to generate object segmentation from user inputs in the form of extreme points and corrective clicks. We propose a novel approach for effectively encoding the user input from extreme points and corrective clicks, in a novel and scalable manner that allows the network to work with a variable number of clicks, including corrective clicks for output refinement. We also integrate a dual attention module with our approach to increase the efficacy of the model in preferentially attending to the objects. We demonstrate that these additions help achieve significant improvements over state-of-the-art in dense object segmentation from user inputs, on multiple large-scale datasets. Through experiments, we demonstrate our method's ability to generate high-quality training data as well as its scalability in incorporating extreme points, guiding clicks, and corrective clicks in a principled manner.","tags":["deep learning","computer vision","extreme points","segmentation"],"title":"FAIRS: Soft Focus Generator and Attention for Robust Object Segmentation from Extreme Points","type":"publication"},{"authors":["Ahmed H. Shahin","Shadab Khan","Javier Villafruela","Jianbing Shen","Ling Shao"],"categories":[],"content":"","date":1570665600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570665600,"objectID":"985478004e1f7ae04fa2cbf357d3a0c5","permalink":"/publication/miccai19/","publishdate":"2022-02-28T16:59:59.637688Z","relpermalink":"/publication/miccai19/","section":"publication","summary":"To automate the process of segmenting an anatomy of interest, we can learn a model from previously annotated data. The learning-based approach uses annotations to train a model that tries to emulate the expert labeling on a new data set. While tremendous progress has been made using such approaches, labeling of medical images remains a time-consuming and expensive task. In this paper, we evaluate the utility of extreme points in learning to segment. Specifically, we propose a novel approach to compute a confidence map from extreme points that quantitatively encodes the priors derived from extreme points. We use the confidence map as a cue to train a deep neural network based on ResNet-101 and PSP module to develop a class-agnostic segmentation model that outperforms state-of-the-art method that employs extreme points as a cue. Further, we evaluate a realistic use-case by using our model to generate training data for supervised learning (U-Net) and observed that U-Net performs comparably when trained with either the generated data or the ground truth data. These findings suggest that models trained using cues can be used to generate reliable training data.","tags":["deep learning","extreme points","interactive segmentation"],"title":"Extreme Points Derived Confidence Map as a Cue for Class-Agnostic Interactive Segmentation Using Deep Neural Network","type":"publication"},{"authors":["Ahmed H. Shahin","Karim Amer","Mustafa A. Elattar"],"categories":[],"content":"","date":1562803200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562803200,"objectID":"e886b261b55b3f8fbbd38eaab4a00ff7","permalink":"/publication/isbi19/","publishdate":"2022-02-28T16:59:59.637688Z","relpermalink":"/publication/isbi19/","section":"publication","summary":"The prevalence of skin melanoma is rapidly increasing as well as the recorded death cases of its patients. Automatic image segmentation tools play an important role in providing standardized computer-assisted analysis for skin melanoma patients. Current state-of-the-art segmentation methods are based on fully convolutional neural networks, which utilize an encoder-decoder approach. However, these methods produce coarse segmentation masks due to the loss of location information during the encoding layers. Inspired by Pyramid Scene Parsing Network (PSP-Net), we propose an encoder-decoder model that utilizes pyramid pooling modules in the deep skip connections which aggregate the global context and compensate for the lost spatial information. We trained and validated our approach using ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection grand challenge dataset. Our approach showed a validation accuracy with a Jaccard index of 0.837, which outperforms U-Net. We believe that with this reported reliable accuracy, this method can be introduced for clinical practice.","tags":["skin lesions","segmentation","melanoma","convolutional neural networks","pyramid pooling modules"],"title":"Deep Convolutional Encoder-Decoders with Aggregated Multi-Resolution Skip Connections for Skin Lesion Segmentation","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  **Two**  Three   A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Ahmed H. Shahin","Ahmed Kamal","Mustafa A. Elattar"],"categories":[],"content":"","date":1545436800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545436800,"objectID":"ee53e2fffc1f3f52e61b313e229f7348","permalink":"/publication/cibec/","publishdate":"2022-02-28T16:59:59.637688Z","relpermalink":"/publication/cibec/","section":"publication","summary":"Skin cancer is one of the leading causes of death globally. Early diagnosis of skin lesion significantly increases the prevalence of recovery. Automatic classification of the skin lesion is a challenging task to provide clinicians with the ability to differentiate between different kind of lesion categories and recommend the suitable treatment. Recently, Deep Convolutional Neural Networks have achieved tremendous success in many machine learning applications and have shown an outstanding performance in various computer-assisted diagnosis applications. Our goal is to develop an automated framework that efficiently performs a reliable automatic lesion classification to seven skin lesion types. In this work, we propose a deep neural network-based framework that follows an ensemble approach by combining ResNet-50 and Inception V3 architectures to classify the seven different skin lesion types. Experimental validation results have achieved accurate classification with an assuring validation accuracy up to 0.899.","tags":["skin lesions","melanoma","convolutional neural networks"],"title":"Deep Ensemble Learning for Skin Lesion Classification from Dermoscopic Images","type":"publication"},{"authors":null,"categories":null,"content":"\r  I am serving as a program committee member for the Medical Imaging Meets NeurIPS workshop. Call for abstracts\n  I am starting a research internship at Siemens Healthineers, Princeton, USA.\n  Our abstract on Machine Learning for Identifying IPF Imaging Biomarkers has been accepted for presentation at the annual meeting of the Radiological Society of North America (RSNA'22).\n  Our paper on Prognostic Imaging Biomarker Discovery in Survival Analysis for Idiopathic Pulmonary Fibrosis has been accepted for publication at the Medical Image Computing and Computer Assisted Intervention conference (MICCAI'22).\n  I am serving as a board member in the RISE-MICCAI network. Please feel free to join RISE efforts by filling this form.\n  Our paper on Survival Analysis for Idiopathic Pulmonary Fibrosis using CT Images and Incomplete Clinical Data has been accepted for publication at the Medical Imaging with Deep Learning conference (MIDL'22), with oral recommendation.\n  OSIC has been featured in Wired magazine (link).\n  ","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"a0812ae5f3c926fea6faf4472cefc8e2","permalink":"/news/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/news/","section":"","summary":"List of news.\r\n","tags":[],"title":"News","type":"page"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]